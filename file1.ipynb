{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "import time\n",
    "import ijson\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to extract data with locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63201091459\n",
      "166476331\n"
     ]
    }
   ],
   "source": [
    "# Recording only the tweets where places are given.\n",
    "f=open('mnt/ext100/twitter-huge.json', encoding='utf-8')\n",
    "g=open('twitter-place-data.json','w',encoding='utf-8')\n",
    "g.write('[\\n')\n",
    "f.seek(0,os.SEEK_END)\n",
    "size=f.tell()\n",
    "print(size)\n",
    "f.seek(0)\n",
    "metadata=f.readline()\n",
    "nrows=int(metadata.split(':')[1].split(',')[0]) # number of rows given in the file are wrong.\n",
    "print(nrows)\n",
    "nrows=nrows//10000\n",
    "# nrows=500\n",
    "try:\n",
    "    for i in range(nrows):\n",
    "        data=f.readline()\n",
    "        try:\n",
    "            data=json.loads(data[:-2])\n",
    "        except:\n",
    "            print(f'Error in row and location:{i} {f.tell()}')\n",
    "            if f.tell()>=size:\n",
    "                break\n",
    "        # print(data['doc'].keys())\n",
    "        else:\n",
    "            if 'includes' in data['doc'].keys():\n",
    "                last=data\n",
    "                # pass\n",
    "                # print(data)\n",
    "                g.write(json.dumps(data)+',\\n')\n",
    "    # for key,value in data.items():\n",
    "    #     print(key, value)\n",
    "except:\n",
    "    print(f'File at location {f.tell()}')\n",
    "g.write(json.dumps(last)+'\\n')\n",
    "g.write(']')\n",
    "g.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code saves tweets with covid in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding JSON database to couchDB\n",
    "# Number of tweets in couchDB file=3236320\n",
    "import couchdb\n",
    "import json\n",
    "keyterms=['covid', 'covid-19', 'coronavirus', 'covid-vaccine']\n",
    "dbname='tweetscovid'\n",
    "dbaddress='http://admin:Royai99@127.0.0.1:5984/' #change the address to the couchdb server\n",
    "couch = couchdb.Server(dbaddress)\n",
    "if dbname in couch:\n",
    "    del couch[dbname]\n",
    "    db=couch.create(dbname)\n",
    "else:\n",
    "    db=couch.create(dbname)\n",
    "filename='twitter-place-data.json'\n",
    "filename='C:/Users/Kunal Patel/D folder/_Master_data_science/Cluster and Cloud Computing/assignment 2/twitter-profane.json'\n",
    "jsonfile=open(filename,'r', encoding='utf-8')\n",
    "i=1\n",
    "data_array=[]\n",
    "for row in jsonfile:\n",
    "        try:\n",
    "            data = json.loads(row[:-2])\n",
    "        except:\n",
    "            print(f'This row can\\'t be converted: {row}')\n",
    "        else:\n",
    "            tokens=data['tokens']\n",
    "            tokenlist=tokens.split(\"|\")\n",
    "            for word in tokenlist:\n",
    "                if word.lower() in keyterms:\n",
    "                    data_array.append(data)\n",
    "                    break\n",
    "        if(len(data_array))==100:\n",
    "             db.update(data_array)\n",
    "             data_array=[]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to save tweets with profanity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import couchdb\n",
    "import json\n",
    "\n",
    "filename='twitter-profane.json'\n",
    "jsonfile=open(filename,'r', encoding='utf-8')\n",
    "dbname='tweetsprofane'\n",
    "dbaddress='http://admin:Royai99@127.0.0.1:5984/'\n",
    "couch = couchdb.Server(dbaddress)\n",
    "if dbname in couch:\n",
    "    del couch[dbname]\n",
    "    db=couch.create(dbname)\n",
    "else:\n",
    "    db=couch.create(dbname)\n",
    "i=-1\n",
    "data_array=[]\n",
    "for row in jsonfile:\n",
    "    try:\n",
    "        data = json.loads(row[:-2])\n",
    "        \n",
    "    except:\n",
    "        print(f'This row can\\'t be converted: {row}')\n",
    "    else:\n",
    "        data_array.append(data)\n",
    "        \n",
    "    if(len(data_array)==1000):\n",
    "        db.update(data_array)\n",
    "        data_array=[]\n",
    "\n",
    "if len(data_array)!=0:\n",
    "    db.update(data_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to get formatted data in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process sal.json:0.19279050827026367\n",
      "This row can't be converted: [\n",
      "\n",
      "This row can't be converted: {\"id\":\"1557503993508950016\",\"key\":[2022,8,10,\"988404415789846530\",\"2762475846\",\"1557503993508950016\"],\"value\":{\"tags\":\"\",\"tokens\":\"\"},\"doc\":{\"_id\":\"1557503993508950016\",\"_rev\":\"1-abe35fd2ab64f7349e20324be877be62\",\"data\":{\"author_id\":\"2762475846\",\"conversation_id\":\"988404415789846530\",\"created_at\":\"2022-08-10T23:07:43.000Z\",\"entities\":{\"mentions\":[{\"start\":0,\"end\":15,\"username\":\"BunburyWeather\",\"id\":\"2789736120\"},{\"start\":16,\"end\":27,\"username\":\"Sparrow_65\",\"id\":\"125515218\"},{\"start\":28,\"end\":39,\"username\":\"weather_wa\",\"id\":\"986092014\"},{\"start\":40,\"end\":47,\"username\":\"baxlex\",\"id\":\"179511988\"},{\"start\":48,\"end\":61,\"username\":\"perthovalman\",\"id\":\"42854780\"},{\"start\":62,\"end\":74,\"username\":\"ManjitK6987\",\"id\":\"775918317764644864\"},{\"start\":75,\"end\":90,\"username\":\"ImTheOnlyAstro\",\"id\":\"500446308\"},{\"start\":91,\"end\":99,\"username\":\"RKMac65\",\"id\":\"461533861\"},{\"start\":100,\"end\":114,\"username\":\"TrixieBelden_\",\"id\":\"1224956765474709504\"},{\"start\":115,\"end\":122,\"username\":\"drstip\",\"id\":\"258298063\"},{\"start\":123,\"end\":134,\"username\":\"2017Ferret\",\"id\":\"857774997724475392\"},{\"start\":135,\"end\":142,\"username\":\"paulmp\",\"id\":\"22744846\"},{\"start\":143,\"end\":159,\"username\":\"Richard_Kreider\",\"id\":\"1674489318\"},{\"start\":160,\"end\":176,\"username\":\"sivideoaviation\",\"id\":\"948503715937361920\"},{\"start\":177,\"end\":189,\"username\":\"Barnsy_Lisa\",\"id\":\"29326170\"},{\"start\":190,\"end\":202,\"username\":\"BigV2011WCE\",\"id\":\"191761621\"},{\"start\":203,\"end\":215,\"username\":\"wiccewicker\",\"id\":\"827791722209898496\"},{\"start\":216,\"end\":226,\"username\":\"Rob_lebob\",\"id\":\"1211227922075246594\"},{\"start\":227,\"end\":235,\"username\":\"TheWAWG\",\"id\":\"176041453\"},{\"start\":236,\"end\":250,\"username\":\"aussie_robbob\",\"id\":\"4374920592\"},{\"start\":251,\"end\":263,\"username\":\"WendyBirdOZ\",\"id\":\"2195966832\"},{\"start\":264,\"end\":275,\"username\":\"N8aviation\",\"id\":\"1218828602810105857\"},{\"start\":276,\"end\":285,\"username\":\"perthobs\",\"id\":\"3229987878\"}]},\"geo\":{\"place_id\":\"0118c71c0ed41109\"},\"lang\":\"und\",\"public_metrics\":{\"retweet_count\":0,\"reply_count\":0,\"like_count\":0,\"quote_count\":0},\"text\":\"@BunburyWeather @Sparrow_65 @weather_wa @baxlex @perthovalman @ManjitK6987 @ImTheOnlyAstro @RKMac65 @TrixieBelden_ @drstip @2017Ferret @paulmp @Richard_Kreider @sivideoaviation @Barnsy_Lisa @BigV2011WCE @wiccewicker @Rob_lebob @TheWAWG @aussie_robbob @WendyBirdOZ @N8aviation @perthobs \\ud83d\\udc4d\\ud83d\\udc4d\\ud83d\\ude03\\ud83d\\ude03\",\"sentiment\":0},\"includes\":{\"places\":[{\"full_name\":\"Perth, Western Australia\",\"geo\":{\"type\":\"Feature\",\"bbox\":[115.617614368,-32.675715325,116.239023008,-31.6244855145],\"properties\":{}},\"id\":\"0118c71c0ed41109\"}]},\"matching_rules\":[{\"id\":\"1557388137877635072\",\"tag\":\"\"}]}}\n",
      "\n",
      "This row can't be converted: ]\n"
     ]
    }
   ],
   "source": [
    "import couchdb\n",
    "import json\n",
    "import time\n",
    "from profanity_check import predict, predict_prob\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "start_sal=time.time()\n",
    "with open('sal.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Create a list of dictionaries from the nested dictionary where we include all greater cities including 8acte, 7gdar ( total 9)\n",
    "greaterCityKeys={'sydney':'1gsyd','melbourne':'2gmel','brisbane':'3gbri', 'adelaide':'4gade','perth':'5gper','hobart':'6ghob','darwin':'7gdar','canberra':'8acte'}\n",
    "\n",
    "greaterRegionAbbrev={'new south wales':'nsw', 'south australia': 'sa', 'queensland':'qld', 'victoria':'vic', \n",
    "                     'australian capital territory':'act', 'western australia':'wa','northern territory':'nt',\n",
    "                     'tasmania':'tas'}\n",
    "greaterRegionKeys={'new south wales':'1gsyd', 'south australia': '4gade', 'queensland':'3gbri', 'victoria':'2gmel', \n",
    "                     'australian capital territory':'8acte', 'western australia':'5gper','northern territory':'7gdar',\n",
    "                     'tasmania':'6ghob'}\n",
    "data_list = []\n",
    "cityWithCodesDict={}\n",
    "for key, value in json_data.items():\n",
    "    # if value['gcc'][1]!='r':\n",
    "        if len(key.split(','))==1:\n",
    "            # print(cityWithCodesDict)\n",
    "            cityName=key.split(' (')[0]\n",
    "            if cityName in cityWithCodesDict.keys():\n",
    "                cityWithCodesDict[cityName].append(value['gcc'])\n",
    "                cityWithCodesDict[cityName]=list(set(cityWithCodesDict[cityName]))\n",
    "            else:\n",
    "                cityWithCodesDict[cityName]=[value['gcc']]\n",
    "            \n",
    "            if len(key.split(' ('))>1:\n",
    "                name=key.split(' (')[1]\n",
    "                if len(name.split(' - '))>1:\n",
    "                    name=name.split(' - ')[0]\n",
    "                    if name in cityWithCodesDict.keys():\n",
    "                        pass\n",
    "                    else:\n",
    "                        cityWithCodesDict[name]=[value['gcc']]\n",
    "\n",
    "end_sal=time.time()\n",
    "print(f\"Time to process sal.json:{end_sal-start_sal}\")\n",
    "\n",
    "def matchLocations(tweetLocation):\n",
    "    tweetLocations=tweetLocation.split(', ')        # divide specific and greater region\n",
    "    exactLocation=tweetLocations[0]\n",
    "    \n",
    "    if exactLocation=='australia':\n",
    "        return 'au'\n",
    "\n",
    "    if exactLocation in greaterCityKeys.keys():     # if exact location is the greater city\n",
    "        return greaterCityKeys[exactLocation]\n",
    "    \n",
    "    if exactLocation in greaterRegionKeys.keys():\n",
    "        return ''\n",
    "    \n",
    "\n",
    "    if len(tweetLocations)>1:\n",
    "        greaterRegion=tweetLocations[1]\n",
    "        if greaterRegion in greaterCityKeys.keys():              # see if the greater region is a capital city\n",
    "            return greaterCityKeys[greaterRegion]\n",
    "   \n",
    "   # If there is no name of greater city in the location name\n",
    "    if exactLocation in cityWithCodesDict.keys():\n",
    "        # print('found a match')\n",
    "        allPossibleRegion=cityWithCodesDict[exactLocation]\n",
    "\n",
    "        # print(allPossibleRegion)\n",
    "        if len(allPossibleRegion)==1:\n",
    "            return allPossibleRegion[0]\n",
    "            \n",
    "        elif len(allPossibleRegion)>1:\n",
    "            if len(tweetLocations)<2:\n",
    "                return ''\n",
    "            else:\n",
    "                # print('Searching for greater regions')\n",
    "                if greaterRegion in greaterRegionKeys.keys():\n",
    "                    # print('found it')\n",
    "                    if greaterRegionKeys[greaterRegion] in allPossibleRegion:\n",
    "                        return greaterRegionKeys[greaterRegion]\n",
    "                    else:\n",
    "                        return ''\n",
    "                else:\n",
    "                    return ''\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "\n",
    "filename='twitter-place-data.json'\n",
    "filename='C:/Users/Kunal Patel/D folder/_Master_data_science/Cluster and Cloud Computing/twitter-place-data.json'\n",
    "jsonfile=open(filename,'r', encoding='utf-8')\n",
    "writefile=open('twitter-profane.json', 'w', encoding='utf-8')\n",
    "i=-1\n",
    "for row in jsonfile:\n",
    "        try:\n",
    "            data = json.loads(row[:-2])\n",
    "            \n",
    "        except:\n",
    "            print(f'This row can\\'t be converted: {row}')\n",
    "        else:\n",
    "            try:\n",
    "                place=data['doc']['includes']['places'][0]\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    locationname=place['full_name'].lower()    # get the exact location name\n",
    "                    # print(locationname)\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    code=matchLocations(locationname)\n",
    "                    # print(code)\n",
    "                    if code is None:\n",
    "                        continue\n",
    "                    time=data['doc']['data']['created_at']\n",
    "                    text=data['doc']['data']['text']\n",
    "                    sentiment=data['doc']['data']['sentiment']\n",
    "                    tokens=data['value']['tokens']\n",
    "                    # hottokens=get_hotwords(text)\n",
    "                    profanity=predict_prob([text])[0]\n",
    "                    db_entry={\n",
    "                        'place':place,\n",
    "                        'region_code': code,\n",
    "                        'time':time,\n",
    "                        # 'text':text,\n",
    "                        'sentiment':sentiment,\n",
    "                        'tokens':tokens,\n",
    "                        'profanity':profanity\n",
    "                    }\n",
    "                    writefile.write(json.dumps(db_entry)+',\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save income data to couchdb server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'Feature', 'properties': {'gccsa_code': '1RNSW', 'median_aud': 45798.0, 'gccsa_name': 'Rest of NSW'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.1RNSW'}, {'type': 'Feature', 'properties': {'gccsa_code': '1GSYD', 'median_aud': 52665.0, 'gccsa_name': 'Greater Sydney'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.1GSYD'}, {'type': 'Feature', 'properties': {'gccsa_code': '2RVIC', 'median_aud': 44967.0, 'gccsa_name': 'Rest of Vic.'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.2RVIC'}, {'type': 'Feature', 'properties': {'gccsa_code': '2GMEL', 'median_aud': 50648.0, 'gccsa_name': 'Greater Melbourne'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.2GMEL'}, {'type': 'Feature', 'properties': {'gccsa_code': '3GBRI', 'median_aud': 51346.0, 'gccsa_name': 'Greater Brisbane'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.3GBRI'}, {'type': 'Feature', 'properties': {'gccsa_code': '3RQLD', 'median_aud': 46385.0, 'gccsa_name': 'Rest of Qld'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.3RQLD'}, {'type': 'Feature', 'properties': {'gccsa_code': '4RSAU', 'median_aud': 43998.0, 'gccsa_name': 'Rest of SA'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.4RSAU'}, {'type': 'Feature', 'properties': {'gccsa_code': '4GADE', 'median_aud': 49556.0, 'gccsa_name': 'Greater Adelaide'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.4GADE'}, {'type': 'Feature', 'properties': {'gccsa_code': '5GPER', 'median_aud': 53140.0, 'gccsa_name': 'Greater Perth'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.5GPER'}, {'type': 'Feature', 'properties': {'gccsa_code': '5RWAU', 'median_aud': 50559.0, 'gccsa_name': 'Rest of WA'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.5RWAU'}, {'type': 'Feature', 'properties': {'gccsa_code': '6GHOB', 'median_aud': 47770.0, 'gccsa_name': 'Greater Hobart'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.6GHOB'}, {'type': 'Feature', 'properties': {'gccsa_code': '6RTAS', 'median_aud': 43860.0, 'gccsa_name': 'Rest of Tas.'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.6RTAS'}, {'type': 'Feature', 'properties': {'gccsa_code': '7RNTE', 'median_aud': 54418.0, 'gccsa_name': 'Rest of NT'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.7RNTE'}, {'type': 'Feature', 'properties': {'gccsa_code': '7GDAR', 'median_aud': 63404.0, 'gccsa_name': 'Greater Darwin'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.7GDAR'}, {'type': 'Feature', 'properties': {'gccsa_code': '8ACTE', 'median_aud': 64332.0, 'gccsa_name': 'Australian Capital Territory'}, 'id': 'abs_personal_income_total_income_distribution_gccsa_2017_18.8ACTE'}]\n",
      "{'gccsa_code': '1rnsw', 'median_aud': 45798.0, 'gccsa_name': 'Rest of NSW'}\n",
      "{'gccsa_code': '1gsyd', 'median_aud': 52665.0, 'gccsa_name': 'Greater Sydney'}\n",
      "{'gccsa_code': '2rvic', 'median_aud': 44967.0, 'gccsa_name': 'Rest of Vic.'}\n",
      "{'gccsa_code': '2gmel', 'median_aud': 50648.0, 'gccsa_name': 'Greater Melbourne'}\n",
      "{'gccsa_code': '3gbri', 'median_aud': 51346.0, 'gccsa_name': 'Greater Brisbane'}\n",
      "{'gccsa_code': '3rqld', 'median_aud': 46385.0, 'gccsa_name': 'Rest of Qld'}\n",
      "{'gccsa_code': '4rsau', 'median_aud': 43998.0, 'gccsa_name': 'Rest of SA'}\n",
      "{'gccsa_code': '4gade', 'median_aud': 49556.0, 'gccsa_name': 'Greater Adelaide'}\n",
      "{'gccsa_code': '5gper', 'median_aud': 53140.0, 'gccsa_name': 'Greater Perth'}\n",
      "{'gccsa_code': '5rwau', 'median_aud': 50559.0, 'gccsa_name': 'Rest of WA'}\n",
      "{'gccsa_code': '6ghob', 'median_aud': 47770.0, 'gccsa_name': 'Greater Hobart'}\n",
      "{'gccsa_code': '6rtas', 'median_aud': 43860.0, 'gccsa_name': 'Rest of Tas.'}\n",
      "{'gccsa_code': '7rnte', 'median_aud': 54418.0, 'gccsa_name': 'Rest of NT'}\n",
      "{'gccsa_code': '7gdar', 'median_aud': 63404.0, 'gccsa_name': 'Greater Darwin'}\n",
      "{'gccsa_code': '8acte', 'median_aud': 64332.0, 'gccsa_name': 'Australian Capital Territory'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import couchdb\n",
    "dbname='incomedata'\n",
    "dbaddress='http://admin:Royai99@127.0.0.1:5984/' #change the address to the couchdb server\n",
    "couch = couchdb.Server(dbaddress)\n",
    "if dbname in couch:\n",
    "    del couch[dbname]\n",
    "    db=couch.create(dbname)\n",
    "else:\n",
    "    db=couch.create(dbname)\n",
    "filename=\"abs_personal_income_total_income_distribution_gccsa_2017_18-4766895341661114274.json\"\n",
    "jsonfile=open(filename,'r', encoding='utf-8')\n",
    "data=json.load(jsonfile)\n",
    "features=data['features']\n",
    "print(features)\n",
    "for feature in features:\n",
    "    income=feature['properties']\n",
    "    income['gccsa_code']=income['gccsa_code'].lower()\n",
    "    print(income)\n",
    "    db.save(income)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing view from the python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database exists\n",
      "3236320\n"
     ]
    }
   ],
   "source": [
    "import couchdb\n",
    "import json\n",
    "dbname='tweets'\n",
    "dbaddress='http://admin:Royai99@127.0.0.1:5984/'\n",
    "couch = couchdb.Server(dbaddress)\n",
    "if dbname in couch:\n",
    "    db = couch[dbname]\n",
    "    print('Database exists')\n",
    "else:\n",
    "    db=couch.create(dbname)\n",
    "    print('No such database')\n",
    "print(db.info()['doc_count'])\n",
    "for item in db.view('_design/text_extract/_view/new-view'):\n",
    "    print(item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toot processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['account', 'card', 'content', 'created_at', 'edited_at', 'emojis', 'favourites_count', 'filtered', 'id', 'in_reply_to_account_id', 'in_reply_to_id', 'language', 'media_attachments', 'mentions', 'poll', 'reblog', 'reblogs_count', 'replies_count', 'sensitive', 'spoiler_text', 'tags', 'uri', 'url', 'visibility'])\n",
      "['update', 'gopher', 'pellets', 'time', 'hope', 'gophers']\n",
      "[]\n",
      "['time', 'updat', 'gopher', 'pellet', 'hope']\n"
     ]
    }
   ],
   "source": [
    "## Toot processing\n",
    "from nltk.stem import PorterStemmer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import html2text\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'NOUN']  # removed 'ADJ' terms\n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "dbname='toots'\n",
    "dbaddress='http://admin:Royai99@127.0.0.1:5984/'\n",
    "couch = couchdb.Server(dbaddress)\n",
    "if dbname in couch:\n",
    "    del couch[dbname]\n",
    "    db=couch.create(dbname)\n",
    "else:\n",
    "    db=couch.create(dbname)\n",
    "f=open('tootexamples/onetoot.json','r',encoding='utf-8')\n",
    "x=json.load(f)\n",
    "print(x.keys())\n",
    "date=x['created_at']\n",
    "text=x['content']\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "text=h.handle(text).strip(\"\\n \")\n",
    "# print(date, text)\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "sentimentscore=sentiment.polarity_scores(text)\n",
    "hottokens=get_hotwords(text)\n",
    "ps=PorterStemmer()\n",
    "hottokenstems=[]\n",
    "print(hottokenstems)\n",
    "for word in hottokens:\n",
    "    hottokenstems.append(ps.stem(word))\n",
    "hottokenstems=list(set(hottokenstems))\n",
    "print(hottokenstems)\n",
    "data={\n",
    "    'date':date,\n",
    "    'sentiment':sentiment,\n",
    "    'hottokens':hottokenstems\n",
    "}\n",
    "db.save(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kunal Patel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ancient', 'temple', 'homeland', 'god', 'land', 'jewish', 'country', 'israel', 'vault', 'year', 'zionist', 'text', 'sister', 'spain'}\n",
      "ancient\n",
      "temple\n",
      "homeland\n",
      "god\n",
      "land\n",
      "jewish\n",
      "country\n",
      "israel\n",
      "vault\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "new_text = \"\"\"\n",
    "hypothetically ancient 700 year Jewish temple was found another country say Spain Zionist would claim that country ancient sister homeland Israel then probably find some ancient text their vault saying that God gave them that land too\n",
    "\"\"\"\n",
    "output = set(get_hotwords(new_text))\n",
    "print(output)\n",
    "most_common_list = Counter(output).most_common(10)\n",
    "for item in most_common_list:\n",
    "    print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999822]\n",
      "{'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}\n"
     ]
    }
   ],
   "source": [
    "from profanity_check import predict, predict_prob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "text=\"I love fucking people\"\n",
    "print(predict_prob([text]))\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "print(sentiment.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
